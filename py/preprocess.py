#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
This script is used to process the interaction log files.
It will produce consistent formatting, from which information
can be extracted to compute the bias metrics. 

Created on Mon Jul 17 00:29:52 2017

Example of log download in terminal:
    curl -G -o raw_logs10.json https://pull.logentries.com/541b6268-09ef-4845-8353-0303f85bb788/hosts/cd61ef18-8417-4db4-8014-496020eda97e/0709470c-8d7e-4152-a5ff-d023c2d89fd1/ \-d start=-7200000 \-d format=json

    # gets 60 minutes: 3600000
    # gets 75 minutes: 4500000
    # gets 2 hours: 7200000
    # gets 3 hours: 10800000
    # gets 48 hours: 172800000

@author: emilywall
"""

import bias_util
import os
from os import listdir
from os.path import isfile, join
import sys

# separate logs into files for each user with one log per line
def process_file_by_user(directory, in_file_name): 
    f_in = open(directory + in_file_name, 'r')
    print 'Processing ', in_file_name
    user_id = None
    f_out = None
    end_string = '"}\'"},'
    prev_line = ''
    
    # write each separate set of logs to a different file per user
    num_nulls = 0
    num_lines_written = 0
    complete_line = True
    for line in f_in: 
        if '"m"' in line:
            # get the important part of the log
            line = line.replace('\\', '')
            first_index = line.index('"m":') + 5
            if ('LOG' in line): 
                first_index = line.index('LOG') + 5
                
            if (line[-7: ].strip() == end_string):
                line = line[first_index : -5]
                if (complete_line == False): 
                    line = prev_line + line
                    complete_line = True
            elif (line[-6: ].strip() == end_string[: -1]):
                line = line[first_index : -4]
                if (complete_line == False): 
                    line = prev_line + line
                    complete_line = True
            else:
                line = line[first_index : -4]
                complete_line = False

            
            if (complete_line == False):
                prev_line = line
                continue # don't write it yet
            else: # determine which user the log was generated by
                if ('"userId":null' in line): 
                    num_nulls += 1
                    continue
                ind_1 = line.index('"userId":"') + 10
                ind_2 = line.index('"', ind_1 + 1)
                current_id = line[ind_1 : ind_2]
    
                if (current_id == str(bias_util.all_participants[0])):
                    num_lines_written += 1
                else: 
                    print '*** diff id'
    
                # open the file for this user
                if (current_id != user_id):
                    user_id = current_id
                    if (f_out != None): 
                        f_out.close()
                    f_out = open(directory + 'temp_' + user_id + '.json', 'a')
                f_out.write(line + '\n')
                complete_line = True
                

    print '** ' + str(user_id) + ' lines written: ', num_lines_written #not sure if it should print current_id or user_id
    num_lines_written = 0
    if (num_nulls > 0):
        print '  Interactions from "null" user:', num_nulls
    # close remaining open files
    if (f_out != None):
        f_out.close()
    f_in.close()
    
# format the list of logs into comma-separated json array
def process_to_json(in_directory, in_file_name, out_directory, out_file_name):
    f_in = open(in_directory + in_file_name, 'r')
    f_out = open(out_directory + out_file_name, 'w+')
    last_line = None # maintain last line for comma formatting 
    
    f_out.write('[')
    
    # first read in all of the lines
    all_lines = []
    for line in f_in:
        all_lines.append(line)
    
    # sort the lines by time stamp
    all_lines.sort(key = get_date)
        
    # write the lines to the file
    for line in all_lines:
        if (last_line != None): 
            f_out.write(last_line + ',\n')
        last_line = line.strip()
        
    # write the last line and close it
    f_out.write(last_line)
    f_out.write(']')
    f_out.close()
    f_in.close()
    
# get the date from the line
def get_date(line):
    first_index = line.index('"eventTimeStamp":"')
    second_index = line.index('"', first_index + 18)
    date = line[first_index + 18 : second_index]

    return date

# this block is for pre-processing
# process each exported set of logs from logentries and create separate subdirectories for each user
if __name__ == '__main__': 
    orig_log_files = all_files = [f for f in listdir(bias_util.directory) if ('.json' in f and isfile(join(bias_util.directory, f)))]
    subdirectories = ['logs', 'plots']
    plot_subdirectories = ['all', 'fixed', 'classification_v1', 'classification_v2', 'category_v1', 'category_v2']
    for i in range(0, len(orig_log_files)):
        process_file_by_user(bias_util.directory, orig_log_files[i])

    all_files = [f for f in listdir(bias_util.directory) if ('temp_' in f and isfile(join(bias_util.directory, f)))]
    for i in range(0, len(all_files)):
        user_id = all_files[i][5:].replace('.json', '')
        new_directory = bias_util.directory + 'user_' + user_id + '/'
        
        # create the subdirectory structure
        if not os.path.exists(new_directory):
            os.makedirs(new_directory)    
        for j in range(0, len(subdirectories)):
            if not os.path.exists(new_directory + subdirectories[j] + '/'):
                os.makedirs(new_directory + subdirectories[j] + '/')
        for j in range(0, len(plot_subdirectories)):
            if not os.path.exists(new_directory + 'plots/' + plot_subdirectories[j] + '/'):
                os.makedirs(new_directory + 'plots/' + plot_subdirectories[j] + '/')
                
        new_file_name = 'interactions_' + user_id + '.json'
        process_to_json(bias_util.directory, all_files[i], new_directory + 'logs/', new_file_name)
        os.remove(bias_util.directory + all_files[i]) # comment out this line to keep the temporary files for debugging purposes
        print 'Processed ', new_file_name